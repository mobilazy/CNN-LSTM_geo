{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aba4505",
   "metadata": {},
   "source": [
    "CNN-LSTM Geothermal BHE Analysis\n",
    "\n",
    "Repository: [GitHub Project](https://github.com/yourusername/CNN-LSTM_geo)\n",
    "\n",
    "Files Included:\n",
    "- `Traindata_geothermal_HybridCNNLSTM_rev10_final.py` - Production code with OE401 correction and DST handling\n",
    "- `CodeExplanation_JupyterNotebook.ipynb` - This explanatory notebook\n",
    "- `input/` - Raw sensor data (MeterOE401, MeterOE402, MeterOE403)\n",
    "- `output/` - Model results and performance plots\n",
    "\n",
    "To run: `python Traindata_geothermal_HybridCNNLSTM_rev10_final.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaccd482",
   "metadata": {},
   "source": [
    "Predicting Geothermal Heat Pump Performance with Deep Learning\n",
    "\n",
    "This notebook walks through the analysis performed in the master's thesis on using deep learning to predict how well different types of ground heat exchangers work.\n",
    "\n",
    "What This Research Is About\n",
    "\n",
    "The University of Stavanger has a ground-source heat pump system with 120 boreholes drilled 300 meters deep into the ground. Each borehole has pipes that circulate fluid to extract heat from the ground. The question is: can we accurately predict the temperature coming back from these boreholes?\n",
    "\n",
    "Why does this matter? Better predictions mean better system control, more efficient operation, and lower energy costs.\n",
    "\n",
    "Three Types of Heat Exchangers\n",
    "\n",
    "The study compares three different pipe configurations, all at the same depth:\n",
    "\n",
    "- Single U-tube (45mm diameter): 112 production wells serving the campus\n",
    "- Double U-tube (45mm diameter): 4 research wells with twice the pipe surface area\n",
    "- MuoviEllipse (63mm diameter): 4 research wells with an elliptical cross-section\n",
    "\n",
    "The Model\n",
    "\n",
    "A CNN-LSTM neural network learns from past temperature, flow rate, and power data to predict future outlet temperatures. The model achieved impressive accuracy: average error of just 0.17°C when predicting temperatures 21 days ahead.\n",
    "\n",
    "What You'll See\n",
    "\n",
    "- How the data was loaded and corrected\n",
    "- Quality checks and cleaning steps\n",
    "- How all three configurations were combined into one training dataset\n",
    "- Performance results showing the model works well for all three types\n",
    "- What these results tell us about geothermal system design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c331c67b",
   "metadata": {},
   "source": [
    "1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c8524",
   "metadata": {},
   "source": [
    "2. Loading the Data\n",
    "\n",
    "Two Data Issues That Needed Fixing\n",
    "\n",
    "Timestamp Handling\n",
    "\n",
    "When daylight saving time transitions happen, some timestamps appear twice in the data. The code uses timezone-aware parsing with `ambiguous=False` and `nonexistent=\"shift_forward\"` to handle these cases correctly.\n",
    "\n",
    "Overlapping Measurements\n",
    "\n",
    "The OE401 sensor measures all 120 boreholes together. However, this includes 8 research wells that are also measured separately:\n",
    "- 4 MuoviEllipse wells (OE402 sensor)\n",
    "- 4 Double U-tube wells (OE403 sensor)\n",
    "\n",
    "To get accurate per-well values for the 112 production wells, we need to:\n",
    "1. Subtract the 8 research wells from the OE401 total\n",
    "2. Divide by 112 (not 120)\n",
    "\n",
    "This correction ensures we're not double-counting the research wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sensor_data_with_dst_handling(csv_path, bhe_type, normalize_by_wells=None):\n",
    "    \"\"\"\n",
    "    Load sensor data with comprehensive DST handling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_path : str\n",
    "        Path to CSV file\n",
    "    bhe_type : str\n",
    "        Configuration identifier (single_u45mm, double_u45mm, muovi_ellipse_63mm)\n",
    "    normalize_by_wells : int or None\n",
    "        If provided, normalize power and flow by this number (deferred for OE401)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with timezone-aware timestamps\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"Data file not found: {csv_path}\")\n",
    "    \n",
    "    print(f\"\\nLoading {bhe_type} data from: {os.path.basename(csv_path)}\")\n",
    "    \n",
    "    df = pd.read_csv(csv_path, encoding='utf-8', sep=';', decimal=',')\n",
    "    print(f\"  Raw shape: {df.shape}\")\n",
    "    \n",
    "    # Step 1: Convert Timestamp from object to datetime with DST handling\n",
    "    print(\"  Step 1: Converting timestamps with DST awareness...\")\n",
    "    s = df[\"Timestamp\"].astype(str)\n",
    "    s_clean = (\n",
    "        s.str.replace(\"\\u00A0\", \" \", regex=False)  # remove non-breaking spaces\n",
    "         .str.strip()\n",
    "    )\n",
    "    \n",
    "    ts_parsed = pd.to_datetime(\n",
    "        s_clean,\n",
    "        dayfirst=True,      # Norwegian format: dd.mm.yyyy HH:MM\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    \n",
    "    df[\"Timestamp\"] = ts_parsed\n",
    "    \n",
    "    # Step 2: Apply timezone localization with DST transition handling\n",
    "    print(\"  Step 2: Localizing to Europe/Oslo timezone with DST resolution...\")\n",
    "    df[\"Timestamp\"] = df[\"Timestamp\"].dt.tz_localize(\n",
    "        \"Europe/Oslo\",\n",
    "        ambiguous=False,            # Fall DST: choose second occurrence (standard time)\n",
    "        nonexistent=\"shift_forward\" # Spring DST: shift forward to next valid time\n",
    "    )\n",
    "    \n",
    "    # Step 3: Sort by timestamp to ensure temporal order\n",
    "    print(\"  Step 3: Sorting by timestamp...\")\n",
    "    df = df.sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "    \n",
    "    # Remove duplicate timestamps that might still exist\n",
    "    initial_len = len(df)\n",
    "    df = df.drop_duplicates(subset=['Timestamp'], keep='first').reset_index(drop=True)\n",
    "    if len(df) < initial_len:\n",
    "        print(f\"  Removed {initial_len - len(df)} duplicate timestamps\")\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    numeric_cols = ['supply_temp', 'return_temp', 'power_kw', 'flow_rate']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Normalize if specified (for OE402 and OE403, not OE401 yet)\n",
    "    if normalize_by_wells is not None:\n",
    "        print(f\"  Step 4: Normalizing measurements for {normalize_by_wells} wells...\")\n",
    "        if 'power_kw' in df.columns:\n",
    "            df['power_kw'] = df['power_kw'] / normalize_by_wells\n",
    "        if 'flow_rate' in df.columns:\n",
    "            df['flow_rate'] = df['flow_rate'] / normalize_by_wells\n",
    "    \n",
    "    # Add configuration identifier\n",
    "    df['bhe_type'] = bhe_type\n",
    "    \n",
    "    print(f\"  Final shape: {df.shape}\")\n",
    "    print(f\"  Timestamp range: {df['Timestamp'].min()} to {df['Timestamp'].max()}\")\n",
    "    print(f\"  Timestamp dtype: {df['Timestamp'].dtype}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6e3fa4",
   "metadata": {},
   "source": [
    "Load Research Well Data First (OE402 and OE403)\n",
    "\n",
    "We load the research wells first because we need their values to correct the OE401 contamination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee1c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "INPUT_DIR = os.path.join(os.getcwd(), \"input\")\n",
    "\n",
    "# Load MuoviEllipse 63mm research wells (4 wells) - OE402\n",
    "muovi_ellipse_df = load_sensor_data_with_dst_handling(\n",
    "    csv_path=os.path.join(INPUT_DIR, \"MeterOE402_Ellipse63.csv\"),\n",
    "    bhe_type='muovi_ellipse_63mm',\n",
    "    normalize_by_wells=4\n",
    ")\n",
    "\n",
    "# Load Double U45mm research wells (4 wells) - OE403\n",
    "double_u45_df = load_sensor_data_with_dst_handling(\n",
    "    csv_path=os.path.join(INPUT_DIR, \"MeterOE403_doubleU45.csv\"),\n",
    "    bhe_type='double_u45mm',\n",
    "    normalize_by_wells=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72823b36",
   "metadata": {},
   "source": [
    "Load and Correct OE401 Data (Single U45mm Complete Field)\n",
    "\n",
    "Correction Logic (implemented in load_complete_field_data() in production code):\n",
    "\n",
    "1. OE401 raw = sum of 120 wells (including 8 research wells)\n",
    "2. Research wells contribution = OE402 (4 wells) + OE403 (4 wells) = 8 wells\n",
    "3. Clean production field = OE401 - (OE402 + OE403) aggregated values\n",
    "4. Per-well value = Clean production field / 112 wells\n",
    "\n",
    "This removes the contamination and provides accurate per-well measurements for the Single U45mm configuration.\n",
    "\n",
    "The production script performs this correction by:\n",
    "- Loading all three sensor files\n",
    "- Aligning timestamps with DST handling\n",
    "- Calculating total research contribution: (OE402_perwell × 4) + (OE403_perwell × 4)\n",
    "- Subtracting research totals from OE401\n",
    "- Normalizing by 112 production wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf4dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration version for notebook - production code uses load_complete_field_data()\n",
    "# which implements the full correction within the script\n",
    "\n",
    "# Load OE401 without normalization yet\n",
    "single_u45_raw = load_sensor_data_with_dst_handling(\n",
    "    csv_path=os.path.join(INPUT_DIR, \"MeterOE401_singleU45.csv\"),\n",
    "    bhe_type='single_u45mm',\n",
    "    normalize_by_wells=None  # Don't normalize yet\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CORRECTING OE401 DATA FOR RESEARCH WELL CONTAMINATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"This demonstrates the correction implemented in load_complete_field_data()\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find overlapping timestamps across all three sensors\n",
    "common_timestamps = set(single_u45_raw['Timestamp']).intersection(\n",
    "    set(muovi_ellipse_df['Timestamp'])\n",
    ").intersection(\n",
    "    set(double_u45_df['Timestamp'])\n",
    ")\n",
    "\n",
    "print(f\"\\nCommon timestamps across all sensors: {len(common_timestamps):,}\")\n",
    "\n",
    "# Filter to common timestamps\n",
    "single_u45_aligned = single_u45_raw[single_u45_raw['Timestamp'].isin(common_timestamps)].copy()\n",
    "muovi_aligned = muovi_ellipse_df[muovi_ellipse_df['Timestamp'].isin(common_timestamps)].copy()\n",
    "double_aligned = double_u45_df[double_u45_df['Timestamp'].isin(common_timestamps)].copy()\n",
    "\n",
    "# Sort all by timestamp for alignment\n",
    "single_u45_aligned = single_u45_aligned.sort_values('Timestamp').reset_index(drop=True)\n",
    "muovi_aligned = muovi_aligned.sort_values('Timestamp').reset_index(drop=True)\n",
    "double_aligned = double_aligned.sort_values('Timestamp').reset_index(drop=True)\n",
    "\n",
    "# Calculate research well aggregated contribution (re-aggregate from per-well back to totals)\n",
    "# OE402 was divided by 4, OE403 was divided by 4, so multiply back to get totals\n",
    "research_power_total = (muovi_aligned['power_kw'] * 4) + (double_aligned['power_kw'] * 4)\n",
    "research_flow_total = (muovi_aligned['flow_rate'] * 4) + (double_aligned['flow_rate'] * 4)\n",
    "\n",
    "print(f\"\\nResearch wells total power range: {research_power_total.min():.2f} to {research_power_total.max():.2f} kW\")\n",
    "print(f\"Research wells total flow range: {research_flow_total.min():.2f} to {research_flow_total.max():.2f} m³/h\")\n",
    "\n",
    "# Subtract research contribution from OE401 total\n",
    "single_u45_aligned['power_kw'] = single_u45_aligned['power_kw'] - research_power_total\n",
    "single_u45_aligned['flow_rate'] = single_u45_aligned['flow_rate'] - research_flow_total\n",
    "\n",
    "# Now normalize by 112 production wells\n",
    "single_u45_aligned['power_kw'] = single_u45_aligned['power_kw'] / 112\n",
    "single_u45_aligned['flow_rate'] = single_u45_aligned['flow_rate'] / 112\n",
    "\n",
    "print(f\"\\nCorrected OE401 per-well power range: {single_u45_aligned['power_kw'].min():.3f} to {single_u45_aligned['power_kw'].max():.3f} kW\")\n",
    "print(f\"Corrected OE401 per-well flow range: {single_u45_aligned['flow_rate'].min():.3f} to {single_u45_aligned['flow_rate'].max():.3f} m³/h\")\n",
    "\n",
    "# Use corrected data\n",
    "single_u45_df = single_u45_aligned\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA LOADING COMPLETE WITH CORRECTIONS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSingle U45mm (112 production wells): {len(single_u45_df):,} records\")\n",
    "print(f\"Double U45mm (4 research wells): {len(double_u45_df):,} records\")\n",
    "print(f\"MuoviEllipse 63mm (4 research wells): {len(muovi_ellipse_df):,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06df86",
   "metadata": {},
   "source": [
    "3. Checking Data Quality\n",
    "\n",
    "Before cleaning, let's look at what we're working with for each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73425df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df, config_name):\n",
    "    \"\"\"Generate comprehensive data quality report.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Data Quality Assessment: {config_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\nShape: {df.shape}\")\n",
    "    print(f\"Date range: {df['Timestamp'].min()} to {df['Timestamp'].max()}\")\n",
    "    print(f\"Duration: {(df['Timestamp'].max() - df['Timestamp'].min()).days} days\")\n",
    "    \n",
    "    print(\"\\nMissing values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100).round(2)\n",
    "    for col, count in missing.items():\n",
    "        if count > 0:\n",
    "            print(f\"  {col}: {count} ({missing_pct[col]}%)\")\n",
    "    \n",
    "    print(\"\\nDescriptive statistics:\")\n",
    "    numeric_cols = ['supply_temp', 'return_temp', 'power_kw', 'flow_rate']\n",
    "    available_cols = [col for col in numeric_cols if col in df.columns]\n",
    "    print(df[available_cols].describe().round(3))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Assess quality for all configurations\n",
    "assess_data_quality(single_u45_df, \"Single U45mm (112 production wells - corrected)\")\n",
    "assess_data_quality(double_u45_df, \"Double U45mm (4 research wells)\")\n",
    "assess_data_quality(muovi_ellipse_df, \"MuoviEllipse 63mm (4 research wells)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14fe5a5",
   "metadata": {},
   "source": [
    "4. Cleaning the Data\n",
    "\n",
    "The data goes through 8 cleaning steps to remove sensor errors and measurement glitches:\n",
    "\n",
    "1. Remove physically impossible values (e.g., temperatures below -10°C or above 50°C)\n",
    "2. Detect stuck sensors (same value repeated 18+ times)\n",
    "3. Apply median filtering to smooth out noise\n",
    "4. Fill small gaps (up to 20 minutes) with interpolation\n",
    "5. Check that temperature and power measurements make physical sense together\n",
    "6. Remove records with too many missing values\n",
    "7. Final cleanup\n",
    "\n",
    "This keeps the real thermal behavior while filtering out bad measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b00604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bhe_data(df, dataset_name=\"\"):\n",
    "    \"\"\"\n",
    "    8-stage data cleaning pipeline from thesis methodology.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        return df\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Cleaning: {dataset_name}\")\n",
    "    print(f\"Initial records: {len(df):,}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    temp_cols = ['supply_temp', 'return_temp']\n",
    "    power_cols = ['power_kw']\n",
    "    flow_cols = ['flow_rate']\n",
    "    \n",
    "    # Stage 2: Physical constraint validation\n",
    "    print(\"\\nStage 2: Physical constraint validation\")\n",
    "    for col in temp_cols:\n",
    "        if col in df_clean.columns:\n",
    "            invalid = ((df_clean[col] < -10) | (df_clean[col] > 50))\n",
    "            if invalid.sum() > 0:\n",
    "                print(f\"  {col}: Removed {invalid.sum()} out-of-range values\")\n",
    "                df_clean.loc[invalid, col] = np.nan\n",
    "    \n",
    "    for col in power_cols:\n",
    "        if col in df_clean.columns:\n",
    "            invalid = ((df_clean[col] < -500) | (df_clean[col] > 500))\n",
    "            if invalid.sum() > 0:\n",
    "                print(f\"  {col}: Removed {invalid.sum()} out-of-range values\")\n",
    "                df_clean.loc[invalid, col] = np.nan\n",
    "    \n",
    "    for col in flow_cols:\n",
    "        if col in df_clean.columns:\n",
    "            invalid = ((df_clean[col] < 0) | (df_clean[col] > 100))\n",
    "            if invalid.sum() > 0:\n",
    "                print(f\"  {col}: Removed {invalid.sum()} out-of-range values\")\n",
    "                df_clean.loc[invalid, col] = np.nan\n",
    "    \n",
    "    # Stage 3: Sensor anomaly detection (stuck readings)\n",
    "    print(\"\\nStage 3: Sensor anomaly detection (18+ consecutive duplicates)\")\n",
    "    for col in temp_cols + power_cols + flow_cols:\n",
    "        if col in df_clean.columns:\n",
    "            mask = df_clean[col].rolling(window=18, center=False).apply(\n",
    "                lambda x: len(set(x)) == 1, raw=True\n",
    "            ).fillna(0) > 0\n",
    "            if mask.sum() > 0:\n",
    "                print(f\"  {col}: Flagged {mask.sum()} stuck sensor readings\")\n",
    "                df_clean.loc[mask, col] = np.nan\n",
    "    \n",
    "    # Stage 4: Median filtering\n",
    "    print(\"\\nStage 4: Median filtering\")\n",
    "    window_temp = 3\n",
    "    window_other = 5\n",
    "    \n",
    "    for col in temp_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].rolling(window=window_temp, center=True, min_periods=1).median()\n",
    "    \n",
    "    for col in power_cols + flow_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].rolling(window=window_other, center=True, min_periods=1).median()\n",
    "    \n",
    "    print(f\"  Applied median filtering (temps={window_temp}, other={window_other})\")\n",
    "    \n",
    "    # Stage 5: Gap interpolation (max 4 consecutive readings = 20 minutes)\n",
    "    print(\"\\nStage 5: Gap interpolation (max 4 consecutive readings)\")\n",
    "    max_gap = 4\n",
    "    for col in temp_cols + power_cols + flow_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].interpolate(method='linear', limit=max_gap, limit_direction='forward')\n",
    "    \n",
    "    # Stage 6: Physics validation (temperature-power consistency)\n",
    "    print(\"\\nStage 6: Physics validation (temperature-power consistency)\")\n",
    "    if all(col in df_clean.columns for col in ['supply_temp', 'return_temp', 'power_kw']):\n",
    "        temp_diff = df_clean['supply_temp'] - df_clean['return_temp']\n",
    "        \n",
    "        # Heat extraction: power > 0, return should be cooler (temp_diff > 0, but not excessive)\n",
    "        invalid_extraction = (df_clean['power_kw'] > 0) & ((temp_diff < 0) | (temp_diff > 8))\n",
    "        \n",
    "        # Heat rejection: power < 0, return should be warmer (temp_diff < 0, but not excessive)\n",
    "        invalid_rejection = (df_clean['power_kw'] < 0) & ((temp_diff > 0) | (temp_diff < -8))\n",
    "        \n",
    "        invalid_physics = invalid_extraction | invalid_rejection\n",
    "        \n",
    "        if invalid_physics.sum() > 0:\n",
    "            print(f\"  Removed {invalid_physics.sum()} physics-inconsistent readings\")\n",
    "            df_clean.loc[invalid_physics, ['supply_temp', 'return_temp', 'power_kw']] = np.nan\n",
    "    \n",
    "    # Stage 7: Missing data threshold filtering\n",
    "    print(\"\\nStage 7: Missing data threshold filtering\")\n",
    "    essential_cols = temp_cols + power_cols + flow_cols\n",
    "    available_cols = [col for col in essential_cols if col in df_clean.columns]\n",
    "    \n",
    "    if dataset_name.lower().startswith('single'):\n",
    "        missing_threshold = 0.25\n",
    "    else:\n",
    "        missing_threshold = 0.50\n",
    "    \n",
    "    if available_cols:\n",
    "        missing_pct = df_clean[available_cols].isnull().sum(axis=1) / len(available_cols)\n",
    "        rows_to_drop = missing_pct > missing_threshold\n",
    "        if rows_to_drop.sum() > 0:\n",
    "            print(f\"  Removed {rows_to_drop.sum()} rows exceeding {missing_threshold*100}% missing threshold\")\n",
    "            df_clean = df_clean[~rows_to_drop].copy()\n",
    "    \n",
    "    # Stage 8: Final quality assurance\n",
    "    print(\"\\nStage 8: Final quality assurance\")\n",
    "    initial_len = len(df_clean)\n",
    "    df_clean = df_clean.dropna(subset=available_cols).copy()\n",
    "    final_removed = initial_len - len(df_clean)\n",
    "    \n",
    "    if final_removed > 0:\n",
    "        print(f\"  Removed {final_removed} rows with remaining NaN values\")\n",
    "    \n",
    "    retention_pct = len(df_clean) / len(df) * 100\n",
    "    print(f\"\\nFinal: {len(df_clean):,} records ({retention_pct:.1f}% retained)\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply cleaning to all configurations\n",
    "single_u45_clean = clean_bhe_data(single_u45_df, \"Single U45mm (112 production wells)\")\n",
    "double_u45_clean = clean_bhe_data(double_u45_df, \"Double U45mm (4 research wells)\")\n",
    "muovi_ellipse_clean = clean_bhe_data(muovi_ellipse_df, \"MuoviEllipse 63mm (4 research wells)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902bb000",
   "metadata": {},
   "source": [
    "5. Temperature Distribution Plots\n",
    "\n",
    "These plots show supply vs return temperatures for each configuration after cleaning. Clean data should show a clear relationship without extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096833bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display temperature distribution plots from output folder\n",
    "output_dir = 'output'\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Temperature Distributions by Configuration', fontsize=16, fontweight='bold')\n",
    "\n",
    "image_files = [\n",
    "    ('singleu45_(oe401)_temperature_distribution.png', 'Single U-tube 45mm'),\n",
    "    ('doubleu45_(oe403)_temperature_distribution.png', 'Double U-tube 45mm'),\n",
    "    ('muoviellipse_(oe402)_temperature_distribution.png', 'MuoviEllipse 63mm')\n",
    "]\n",
    "\n",
    "for ax, (img_file, title) in zip(axes, image_files):\n",
    "    img_path = os.path.join(output_dir, img_file)\n",
    "    if os.path.exists(img_path):\n",
    "        img = Image.open(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f'Image not found:\\n{img_file}', \n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Each plot shows how supply and return temperatures relate for that configuration.\")print(\"The tight clustering indicates good data quality after cleaning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de340502",
   "metadata": {},
   "source": [
    "6. Combining All Configurations\n",
    "\n",
    "Why Train One Model Instead of Three?\n",
    "\n",
    "All three configurations share the same basic physics: heat flowing through pipes into the ground. By training one model on all the data:\n",
    "\n",
    "- The model learns these shared patterns from more data (especially the 112 production wells)\n",
    "- A `bhe_type` number (0, 1, or 2) tells the model which configuration it's looking at\n",
    "- The model can learn what's different about each configuration while using the shared patterns\n",
    "\n",
    "This is more efficient than training three separate models, and the results show it works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f7186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find overlapping time period across all configurations\n",
    "start_time = max(\n",
    "    single_u45_clean['Timestamp'].min(),\n",
    "    double_u45_clean['Timestamp'].min(),\n",
    "    muovi_ellipse_clean['Timestamp'].min()\n",
    ")\n",
    "\n",
    "end_time = min(\n",
    "    single_u45_clean['Timestamp'].max(),\n",
    "    double_u45_clean['Timestamp'].max(),\n",
    "    muovi_ellipse_clean['Timestamp'].max()\n",
    ")\n",
    "\n",
    "print(f\"Overlapping operational period:\")\n",
    "print(f\"  Start: {start_time}\")\n",
    "print(f\"  End: {end_time}\")\n",
    "print(f\"  Duration: {(end_time - start_time).days} days\")\n",
    "\n",
    "# Filter to overlapping period\n",
    "single_u45_overlap = single_u45_clean[\n",
    "    (single_u45_clean['Timestamp'] >= start_time) & \n",
    "    (single_u45_clean['Timestamp'] <= end_time)\n",
    "].copy()\n",
    "\n",
    "double_u45_overlap = double_u45_clean[\n",
    "    (double_u45_clean['Timestamp'] >= start_time) & \n",
    "    (double_u45_clean['Timestamp'] <= end_time)\n",
    "].copy()\n",
    "\n",
    "muovi_ellipse_overlap = muovi_ellipse_clean[\n",
    "    (muovi_ellipse_clean['Timestamp'] >= start_time) & \n",
    "    (muovi_ellipse_clean['Timestamp'] <= end_time)\n",
    "].copy()\n",
    "\n",
    "# Combine into unified dataset\n",
    "combined_df = pd.concat([\n",
    "    single_u45_overlap,\n",
    "    double_u45_overlap,\n",
    "    muovi_ellipse_overlap\n",
    "], ignore_index=True)\n",
    "\n",
    "# Sort by timestamp\n",
    "combined_df = combined_df.sort_values('Timestamp').reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nUnified dataset shape: {combined_df.shape}\")\n",
    "print(f\"\\nConfiguration distribution:\")\n",
    "print(combined_df['bhe_type'].value_counts())\n",
    "print(f\"\\nSample distribution:\")\n",
    "for bhe_type in combined_df['bhe_type'].unique():\n",
    "    count = (combined_df['bhe_type'] == bhe_type).sum()\n",
    "    pct = count / len(combined_df) * 100\n",
    "    print(f\"  {bhe_type}: {count:,} samples ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nThis unified dataset is the foundation for training a single model across all configurations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e8d8e8",
   "metadata": {},
   "source": [
    "7. Features Used for Prediction\n",
    "\n",
    "The model uses four pieces of information to predict outlet temperature:\n",
    "\n",
    "1. Supply temperature: How warm is the fluid going into the ground?\n",
    "2. Flow rate: How fast is the fluid moving? (per well)\n",
    "3. Power: How much heat is being extracted\\rejected? (per well)\n",
    "4. Configuration type: Which pipe configuration is this? (0=Single, 1=Double, 2=Ellipse)\n",
    "\n",
    "The model looks at the past 4 hours (48 five-minute readings) to predict the next temperature value.\n",
    "\n",
    "Target: Return temperature from the borehole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a71423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode BHE types as categorical numerical values\n",
    "bhe_encoding = {\n",
    "    'single_u45mm': 0,\n",
    "    'double_u45mm': 1,\n",
    "    'muovi_ellipse_63mm': 2\n",
    "}\n",
    "\n",
    "combined_df['bhe_type_encoded'] = combined_df['bhe_type'].map(bhe_encoding)\n",
    "\n",
    "print(\"BHE Type Encoding:\")\n",
    "for bhe_type, code in bhe_encoding.items():\n",
    "    print(f\"  {bhe_type}: {code}\")\n",
    "\n",
    "# Define feature columns for model input\n",
    "feature_cols = ['supply_temp', 'flow_rate', 'power_kw', 'bhe_type_encoded']\n",
    "target_col = 'return_temp'\n",
    "\n",
    "print(f\"\\nInput features: {feature_cols}\")\n",
    "print(f\"Target variable: {target_col}\")\n",
    "\n",
    "# Display feature statistics by configuration\n",
    "print(\"\\nFeature statistics by configuration:\")\n",
    "summary = combined_df.groupby('bhe_type')[['supply_temp', 'return_temp', 'power_kw', 'flow_rate']].describe()\n",
    "print(summary.round(3))\n",
    "\n",
    "print(\"\\nNote: Different configurations show distinct operational characteristics,\")\n",
    "print(\"confirming that the model must learn configuration-specific behaviors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef3488",
   "metadata": {},
   "source": [
    "8. Model Performance Results\n",
    "\n",
    "The comprehensive analysis below shows how well the model predicts temperatures for all three configurations over a 21-day test period.\n",
    "\n",
    "What to look for:\n",
    "- The predicted line should follow the actual measurements closely\n",
    "- MAE (Mean Absolute Error) shows average prediction error in °C\n",
    "- All three configurations maintain errors well below 0.2°C\n",
    "- Different configurations show slightly different accuracy, which makes sense given their different designs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62748da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the comprehensive collector analysis from thesis output\n",
    "try:\n",
    "    img_path = os.path.join('output', 'comprehensive_collector_analysis.png')\n",
    "    if os.path.exists(img_path):\n",
    "        img = Image.open(img_path)\n",
    "        plt.figure(figsize=(18, 14))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title('Comprehensive Collector Analysis - Unified Model Performance', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nPerformance Metrics from Unified Model (21-day test window):\")\n",
    "        print(\"  Single U45mm: MAE ≈ 0.174°C, RMSE ≈ 0.238°C\")\n",
    "        print(\"  Double U45mm: MAE ≈ 0.164°C, RMSE ≈ 0.227°C\")\n",
    "        print(\"  MuoviEllipse 63mm: MAE ≈ 0.178°C, RMSE ≈ 0.241°C\")\n",
    "        print(\"  Overall: MAE ≈ 0.172°C, RMSE ≈ 0.235°C\")\n",
    "        print(\"\\nThese differentiated performance metrics demonstrate that the unified model\")\n",
    "        print(\"successfully learns configuration-specific thermal behaviors while maintaining\")\n",
    "        print(\"consistent sub-degree accuracy across all BHE types.\")\n",
    "    else:\n",
    "        print(f\"Comprehensive analysis plot not found at: {img_path}\")\n",
    "        print(\"Run the main training script first to generate this output.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading image: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fcc571",
   "metadata": {},
   "source": [
    "9. What This Means\n",
    "\n",
    "Key Takeaways\n",
    "\n",
    "Data Quality Matters\n",
    "Correcting the OE401 overlap and handling timestamp issues properly were essential for getting accurate results. Without these corrections, the production well data would have been skewed by including the research wells twice.\n",
    "\n",
    "One Model, Three Configurations\n",
    "The model achieves sub-degree accuracy (average error 0.17°C) for all three configurations. This proves that training one unified model works well, even though each configuration behaves somewhat differently.\n",
    "\n",
    "Performance Differences\n",
    "- Double U-tube: Best prediction accuracy (0.164°C average error)\n",
    "- Single U-tube: Solid baseline performance with most data\n",
    "- MuoviEllipse: Slightly higher error but still excellent (0.178°C)\n",
    "\n",
    "Why This Approach Works\n",
    "\n",
    "The CNN-LSTM architecture captures both spatial patterns (through CNN layers) and time-series patterns (through LSTM layers). Adding the configuration type as an input feature lets the model adjust its predictions for each design while sharing the learned thermal physics across all of them.\n",
    "\n",
    "Practical Benefits\n",
    "- One model is simpler to deploy and maintain than three\n",
    "- The large production field dataset helps improve predictions for the smaller research configurations\n",
    "- Sub-degree accuracy is excellent for geothermal system control and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e412c0",
   "metadata": {},
   "source": [
    "10. Running the Full Training\n",
    "\n",
    "The production script `Traindata_geothermal_HybridCNNLSTM_rev10_final.py` contains all the corrections and complete implementation.\n",
    "\n",
    "To train the model from scratch:\n",
    "\n",
    "```bash\n",
    "python Traindata_geothermal_HybridCNNLSTM_rev10_final.py\n",
    "```\n",
    "\n",
    "What it does:\n",
    "- Loads all three datasets with OE401 correction and DST handling\n",
    "- Applies 8-stage data cleaning pipeline\n",
    "- Trains the CNN-LSTM model (up to 50 epochs with early stopping)\n",
    "- Generates comprehensive performance plots\n",
    "- Saves trained model and metrics\n",
    "\n",
    "Key Differences from Notebook:\n",
    "\n",
    "This notebook demonstrates the concepts, but the production script includes:\n",
    "- Complete 8-stage cleaning pipeline (shown simplified here)\n",
    "- Full CNN-LSTM architecture with optimized hyperparameters\n",
    "- GPU memory optimization and batch size tuning\n",
    "- Comprehensive visualization functions for all metrics\n",
    "- Proper train/validation/test splitting with time-based windows\n",
    "\n",
    "Model Architecture:\n",
    "- Input: 4 hours of data (48 five-minute intervals) × 4 features\n",
    "- CNN layers: Extract spatial patterns (32, 64 channels)\n",
    "- LSTM layers: Capture time-series dependencies (64 hidden units, 2 layers)\n",
    "- Output: Next timestep return temperature\n",
    "\n",
    "Output Files:\n",
    "- `output/comprehensive_collector_analysis.png` - Main results figure with forecast windows\n",
    "- `output/collector_configuration_performance_analysis.png` - Collector performance results\n",
    "- `output/comprehensive_model.pth` - Trained model weights\n",
    "- `output/comprehensive_results.json` - Detailed performance metrics\n",
    "- `output/comprehensive_analysis.log` - Complete training log\n",
    "\n",
    "Training typically takes 15-30 minutes on a GPU, depending on early stopping.\n",
    "\n",
    "Repository Structure for GitHub:\n",
    "```\n",
    "CNN-LSTM_geo/\n",
    "├── Traindata_geothermal_HybridCNNLSTM_rev10_final.py  # Master project code\n",
    "├── CodeExplanation_JupyterNotebook.ipynb              # Explanatory notebook\n",
    "├── input/                                              # Raw sensor data\n",
    "│   ├── MeterOE401_singleU45.csv\n",
    "│   ├── MeterOE402_Ellipse63.csv\n",
    "│   └── MeterOE403_doubleU45.csv\n",
    "└── output/                                             # Generated results\n",
    "    ├── comprehensive_collector_analysis.png\n",
    "    ├── collector_configuration_performance_analysis.png\n",
    "    ├── comprehensive_model.pth\n",
    "    ├── comprehensive_results.json\n",
    "    └── comprehensive_analysis.log\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
